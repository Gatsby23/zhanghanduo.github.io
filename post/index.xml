<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on å¼  ç€š é“ Zhang Handuo</title>
    <link>https://zhanghanduo.github.io/post/</link>
    <description>Recent content in Posts on å¼  ç€š é“ Zhang Handuo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://zhanghanduo.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How to remotely edit your project without having to use VIM</title>
      <link>https://zhanghanduo.github.io/post/remote_edit/</link>
      <pubDate>Tue, 09 Apr 2019 15:20:37 +0800</pubDate>
      
      <guid>https://zhanghanduo.github.io/post/remote_edit/</guid>
      <description>Remotely editing your work when your server does not have public IP address and you don&amp;rsquo;t want to spend any money is not so easy. Maybe you can use Team viewer or Anydesk or even chrome remote desktop, but there are high latencies. Maybe you can use ngrok to remotely ssh to your server, you have to use vim and you are not familiar with it at all ğŸ˜§. I tried to use rmate but it is not convinient to edit across different files in a folder.</description>
    </item>
    
    <item>
      <title>Install new linux environment</title>
      <link>https://zhanghanduo.github.io/post/new_system/</link>
      <pubDate>Mon, 29 Oct 2018 14:46:14 +0800</pubDate>
      
      <guid>https://zhanghanduo.github.io/post/new_system/</guid>
      <description>When you want to install a brand new Ubuntu 16.04 system. You could try to follow this guidance.
 Open Software &amp;amp; Updates and choose the fastest source.
 Update the system:
sudo sh -c &#39;echo &amp;quot;deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main&amp;quot; &amp;gt; /etc/apt/sources.list.d/ros-latest.list&#39; sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get dist-upgrade sudo apt-get install build-essential git  Install Nvidia driver
sudo apt-get install nvidia-396 nvidia-settings  Under cases you have Intel GPU also, please type:</description>
    </item>
    
    <item>
      <title>Price and spec of cloud based GPU</title>
      <link>https://zhanghanduo.github.io/post/gpu_cloud/</link>
      <pubDate>Mon, 29 Oct 2018 14:17:43 +0800</pubDate>
      
      <guid>https://zhanghanduo.github.io/post/gpu_cloud/</guid>
      <description>I summarized several cloud based GPU services:
   Name of services Specification Price (US$)     AWS P2 instance p2.xLarge 0.9 / hour   Azure NC6 1xK80 0.9 / hour   Lambda GPU cloud 8x AWS P2 instances 0.90 / GPU/ hour   NTU HPCC 2 units of 1-P100 is scheduled to be ready by End of October 0.78 / core/ hour    </description>
    </item>
    
    <item>
      <title>You only look once (YOLO) -- (2)</title>
      <link>https://zhanghanduo.github.io/post/yolo2/</link>
      <pubDate>Mon, 20 Aug 2018 16:21:12 +0800</pubDate>
      
      <guid>https://zhanghanduo.github.io/post/yolo2/</guid>
      <description>YOLO has higher localization errors and the recall (measure how good to locate all objects) is lower, compared to SSD. YOLOv2 is the second version of the YOLO with the objective of improving the accuracy significantly while making it faster.
The backbone network architecture of YOLO v2 is as follows: 1. Accuracy Improvements Batch Normalization Also removes the need of dropouts. mAP increases by 2%.
High-resolution Classifier To generate predictions with shape of $7\times 7 \times 125$, we replace the final fully connected layers with a $3\times 3$ convolution layer each outputting 1024 output channels.</description>
    </item>
    
    <item>
      <title>You only look once (YOLO) -- (1)</title>
      <link>https://zhanghanduo.github.io/post/yolo1/</link>
      <pubDate>Mon, 20 Aug 2018 11:39:58 +0800</pubDate>
      
      <guid>https://zhanghanduo.github.io/post/yolo1/</guid>
      <description>You Only Look Once (YOLO) is an object detection system targeted for real-time processing. There are three versions of YOLO: YOLO, YOLOv2 (and YOLO9000) and YOLOv3. For this article, we mainly focus on YOLO first stage.
1. Introduction The target is to find out the bounding box (rectangular boundary frame) of all the objects in the picture and meanwhile judge the categories of them, where left top coordinate denoted by $(x,y)$, as well as the width and height of the rectangle bounding box by $(w,h)$.</description>
    </item>
    
    <item>
      <title>CLion for catkin projects</title>
      <link>https://zhanghanduo.github.io/post/clion/</link>
      <pubDate>Thu, 03 May 2018 10:07:29 +0800</pubDate>
      
      <guid>https://zhanghanduo.github.io/post/clion/</guid>
      <description>Why use CLion?  Better indexing and intelligence hints for C++ than Eclipse and QtCreator-desktop. Free for students. Also integrate PyCharm already. Good Git integration (although I am still used to commandline git). I really like the code inspection clang-tidy function which makes the code style more modern.  Initial set-up Highly recommend you to add source &amp;lt;CATKIN_WORKSPACE_DIR&amp;gt;/devel/setup.bash to the end of ~/.bashrc or ~/.zshrc (Depends you use bash or zsh).</description>
    </item>
    
    <item>
      <title>Pooling Layer in CNN (1)</title>
      <link>https://zhanghanduo.github.io/post/pooling/</link>
      <pubDate>Wed, 02 May 2018 10:16:18 +0800</pubDate>
      
      <guid>https://zhanghanduo.github.io/post/pooling/</guid>
      <description>Today I didn&amp;rsquo;t have the mood to continue my work on map merging of different cameras. So I read the paper from DeepMind of Learned Deformation Stability in Convolutional Neural Networks recommended by Wang Chen.
1. Convolution Operation Convolution operation is typically denoted with an asterisk1: $$ s(t)=(x*w)(t) $$ In Convolutional network terminology, the x is referred to as the input, and the w as the kernel. The output is sometimes referred to as the feature map.</description>
    </item>
    
    <item>
      <title>SVOç›¸å…³é—®é¢˜</title>
      <link>https://zhanghanduo.github.io/post/questions_slam/</link>
      <pubDate>Tue, 03 Apr 2018 18:09:01 +0800</pubDate>
      
      <guid>https://zhanghanduo.github.io/post/questions_slam/</guid>
      <description> ORBSLAM vs SVO
 SVO ï¼ˆå•ç›®ï¼‰
 ä¼˜ç‚¹ï¼š é€Ÿåº¦æå¿«ï¼Œ100å¤šå¸§ï¼Œåœ¨ä½ç«¯è®¡ç®—æœºä¸Šä¹Ÿèƒ½è¾¾åˆ°å®æ—¶æ€§ã€‚è¿½è¸ªå’Œå»ºå›¾ä¸¤ä¸ªçº¿ç¨‹ï¼Œè¿½è¸ªçº¿ç¨‹å’Œptamæˆ–è€…orbslamå¾ˆåƒï¼Œä¹Ÿæ˜¯å»ºç«‹è¯¯å·®é¡¹ï¼Œç„¶årefineå’ŒBAï¼ŒåŒºåˆ«å°±æ˜¯ç”¨çš„ç›´æ¥æ³•çš„Image alignmentè€Œä¸æ˜¯ç‰¹å¾ç‚¹å‡ ä½•ä½ç½®ä¿¡æ¯ã€‚
 ç¼ºç‚¹ï¼šåªæ˜¯é‡Œç¨‹è®¡ï¼Œæ²¡æœ‰åç«¯ä¼˜åŒ–å’Œå›ç¯æ£€æµ‹ï¼Œæ‰€ä»¥ç´¯è®¡è¯¯å·®è¾ƒå¤§ï¼Œè€Œä¸”ä¸€æ—¦ä¸¢äº†å°±æŒ‚äº†ï¼Œæ²¡æ³•é‡å®šä½; è€Œä¸”åœ¨è®¾è®¡çš„æ—¶å€™é’ˆå¯¹çš„æ˜¯ä¿¯è§†çš„æ— äººæœºæ‘„åƒå¤´ï¼Œå¯¹äºå¹³è§†çš„æ‘„åƒæœºæ•ˆæœå¾ˆå·®; æ‹¥æœ‰ç›´æ¥æ³•çš„æ‰€æœ‰ç¼ºç‚¹ï¼šæ€•å…‰ç…§å˜åŒ–ï¼Œæ€•æ¨¡ç³Šï¼Œæ€•å¤§è¿åŠ¨
    svoè¿˜æœ‰ä¸€ä¸ªä¸æ˜¯ç¼ºç‚¹çš„ç¼ºç‚¹ï¼Œå®ƒå¼€æºçš„ä»£ç æœ‰å¥½å¤šå¥½å¤šå‘ï¼Œä½œè€…å¾ˆå¤šç§è´§æ•…æ„æ²¡æœ‰å†™è¿›å¼€æºçš„ä»£ç é‡Œé¢ï¼Œæ‰€ä»¥å®é™…ç”¨çš„æ—¶å€™æœ‰å¾ˆå¤šé—®é¢˜ï¼Œéœ€è¦è‡ªå·±æ ¹æ®æƒ…å†µæ”¹è¿›ã€‚
  OrbSLAM
 ä¼˜ç‚¹ï¼šæ”¯æŒå•ç›®ï¼ŒåŒç›®ï¼ŒRGBDï¼Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ç³»ç»Ÿï¼ŒåŒ…å«äº†é‡Œç¨‹è®¡ï¼Œç‰¹å¾ç‚¹å»ºå›¾BAï¼Œå›ç¯æ£€æµ‹ä¸‰ä¸ªç‹¬ç«‹çº¿ç¨‹ï¼Œåœ¨i7ä¸Šå¤§æ¦‚15ï½20hzï¼ˆè·Ÿè¾“å…¥å›¾åƒå¤§å°ä»¥åŠå‚æ•°è®¾ç½®æœ‰å…³ï¼‰ï¼Œç²¾ç¡®åº¦åœ¨è¿‘å¹´æ¥å±äºæ¯”è¾ƒé«˜çš„äº†ã€‚ç»¼åˆèƒ½åŠ›æœ€å¼ºã€‚
 ç¼ºç‚¹ï¼šORBçš„æå–ä»¥åŠmatchè€—æ—¶è¾ƒå¤§ï¼Œè¿‡å¿«çš„æ—‹è½¬å¯èƒ½ä¼šä¸¢å¤±ã€‚è€Œä¸”å› ä¸ºä¸‰ä¸ªçº¿ç¨‹ä¼šç»™CPUå¸¦æ¥è¾ƒå¤§è´Ÿæ‹…ï¼ŒåŸºæœ¬æ²¡åŠæ³•å†è·‘å…¶ä»–å¤§å‹ç®—æ³•äº†ã€‚å¯¹äºåœºæ™¯ç‰¹å¾ç‚¹ä¸°å¯Œè¦æ±‚é«˜ï¼ŒæŸäº›åœºæ™¯å¦‚æœæ²¡ä»€ä¹ˆç‰¹å¾å¯èƒ½å°±ä¼šå¤±è´¥æˆ–è€…ä¸å‡†ç¡®ã€‚
    èƒ½å¤Ÿè·Ÿorbslam pkçš„æ˜¯æ¯”è¾ƒæ–°çš„DSOç®—æ³•ï¼ŒSVOé™¤äº†é€Ÿåº¦åŸºæœ¬å„ä¸ªæ–¹é¢è¢«åŠæ‰“ã€‚
 Depth acquirement
 1) å…ˆè¯´svoçš„æ·±åº¦æ»¤æ³¢å™¨ï¼Œå±äºæ¸è¿›å¼çš„ä¸‰ç»´é‡å»ºï¼Œä¸æ˜¯å•çº¯çš„æ±‚å–keypointçš„æ·±åº¦ï¼Œè€Œæ˜¯è¦ç»´æŠ¤å€™é€‰ç‚¹seedçš„æ·±åº¦åˆ†å¸ƒï¼Œä»æœªçŸ¥åˆ°ç²—ç•¥åˆ°æ”¶æ•›ï¼Œæ”¶æ•›äº†æ‰æ”¾åˆ°åœ°å›¾ä¸­å…±è¿½è¸ªçº¿ç¨‹ä½¿ç”¨ã€‚å®é™…ä¸Šæ”¶æ•›è¾ƒæ…¢ï¼Œç»“æœä¸¥é‡ä¾èµ–äºå‡†ç¡®çš„ä½å§¿ä¼°è®¡ï¼Œå› æ­¤ç›¸æ¯”äºç‰¹å¾ç‚¹æ³•çš„BAæ²¡æœ‰ä»€ä¹ˆä¼˜åŠ¿ã€‚
 2ï¼‰å†è¯´orbslamçš„BAï¼Œç›¸æ¯”æ¥è¯´è®¡ç®—é‡æ›´å¤§ï¼Œä½†æ˜¯å› ä¸ºæ˜¯frame-frameä»¥åŠlocal mapçš„ä¸¤æ¬¡ä¼˜åŒ–ï¼Œç²¾ç¡®åº¦åº”è¯¥é«˜äºsvoçš„æ»¤æ³¢å™¨ã€‚
  ç°åº¦ä¸å˜æ€§å‡è®¾
ç°åº¦å€¼ä¸å˜æ˜¯å¾ˆå¼ºçš„å‡è®¾ã€‚å¦‚æœç›¸æœºæ˜¯è‡ªåŠ¨æ›å…‰çš„ï¼Œå½“å®ƒè°ƒæ•´æ›å…‰å‚æ•°æ—¶ï¼Œä¼šä½¿å¾—å›¾åƒæ•´ä½“å˜äº®æˆ–å˜æš—ã€‚å…‰ç…§å˜åŒ–æ—¶ä¹Ÿä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚ç‰¹å¾ç‚¹æ³•å¯¹å…‰ç…§å…·æœ‰ä¸€å®šçš„å®¹å¿æ€§ï¼Œè€Œç›´æ¥æ³•ç”±äºè®¡ç®—ç°åº¦é—´çš„å·®å¼‚ï¼Œæ•´ä½“ç°åº¦å˜åŒ–ä¼šç ´åç°åº¦ä¸å˜å‡è®¾ï¼Œä½¿ç®—æ³•å¤±è´¥ã€‚
svoä¸ç›´æ¥å–æŸä¸€ä¸ªåƒç´ ï¼Œè€Œæ˜¯4x4çš„patchï¼Œæˆ‘ä»¬å¯¹å¤šä¸ªç‚¹è¿›è¡Œæœ€å°åŒ–å…‰åº¦è¯¯å·®æ¥è®¡ç®—çš„ã€‚æ‰€ä»¥æ²¡æœ‰æ˜ç¡®çš„thresholdï¼Œå¦‚æœæŸä¸€ä¸¤ä¸ªç‚¹å…‰ç…§å‘ç”Ÿå˜åŒ–è¿˜å¥½ï¼Œå°‘æ•°æœä»å¤šæ•°ã€‚å¦‚æœå‘ç”Ÿå¾®å°å˜åŒ–äº†ï¼Œä½†æ˜¯åªè¦é€‰å–çš„ç‰¹å¾åŒºåŸŸçš„å…‰ç…§è·Ÿå…¶ä»–åŒºåŸŸåŒºåˆ«å¾ˆå¤§ï¼Œä¹Ÿæ˜¯å¯ä»¥å¿å—çš„ï¼Œæ¯•ç«Ÿæœ€å°åŒ–è€Œä¸å¼ºæ±‚å…‰ç…§çš„è¯¯å·®ä¸º0ã€‚æ‰€ä»¥è¯´æœ‰ä¸€å®šçš„é²æ£’æ€§ï¼Œè™½ç„¶è·Ÿç‰¹å¾ç‚¹æ³•æ²¡æ³•æ¯”ã€‚
 VOé™æ­¢çš„æ—¶å€™ï¼Œä¼šæ¼‚å—
ä¸€èˆ¬æƒ…å†µä¸‹VOä¸€èˆ¬ä¸ä¼šæ¼‚çš„ å¦‚æœæ˜¯VIOçš„è¯æ˜¯æœ‰å¯èƒ½çš„ï¼Œä¸€èˆ¬æœ‰å‡ ä¸ªåŸå› :
 1) calibrationåšçš„ä¸å¤Ÿå‡†
 2) åˆå§‹åŒ–çš„æ—¶å€™æ‘„åƒå¤´å°½é‡å†²ç€ç‰¹å¾ç‚¹ä¸°å¯Œçš„åœ°æ–¹ï¼Œå¹¶ç¼“æ…¢ç§»åŠ¨æ‘„åƒå¤´ï¼Œå……åˆ†åˆå§‹åŒ–
    IMUå¦‚æœè´¨é‡ä¸å¥½ä¹Ÿä¼šè½»å¾®çš„æ¼‚ï¼Œä½†æ˜¯ä¸ä¸¥é‡
 </description>
    </item>
    
  </channel>
</rss>